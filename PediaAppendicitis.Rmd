---
title: "Exploring Pediatric Appendicitis"
author: "Muhammad al-Akhdar <muhammad3lakder@gmail.com>"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  fig.align = "center"
)
```

```{r include=FALSE}
#| label: Loading required packages

library(tidyverse)
library(tidymodels)
library(scales)
library(ggthemes)
library(readxl)
library(knitr)
library(kableExtra)
library(janitor)
```


# Abstract

In this analysis, we explore the challenges of diagnosing appendicitis, specifically in pediatrics, by analyzing clinical and ultrasound data. We used a concise data science workflow that included data cleaning, exploratory visualizations, hypothesis testing, and logistic regression modeling. This approach not only revealed key differences in clinical characteristics, but also demonstrated how our analysis can improve healthcare decision making. We also discuss the limitations of the current dataset and methodology, and invite constructive feedback and collaboration for further refinement.



# Introdution

[In 2015, approximately 11.6 million cases of appendicitis were reported, resulting in approximately 50,100 deaths worldwide](https://doi.org/10.1016/S0140-6736(16)31012-1), deaths could be attributed to many reasons, such as lack of access to medicine, quality of care, and medical diagnostic error.

Helping healthcare professionals make more informed decisions is one way to reduce diagnostic errors. 

There are many criteria to do this, such as history taking, physical examination, risk scores (e.g., Alvarado score), and imaging techniques such as ultrasound and CT.

The accuracy of the diagnosis could be improved, for example, by using machine learning. [A recent study built a model to predict appendicitis in pediatrics using an interpretable unsupervised machine learning method](https://doi.org/10.1016/j.media.2023.103042).

[Since many models have been built using only history and physical exam as predictors, we would use the same dataset to first explore the disease and then build models focusing on ultrasound as a diagnostic tool.](https://doi.org/10.5281/zenodo.7711412)



# Methodology

[The dataset was acquired in a retrospective study from a cohort of pediatric patients admitted with abdominal pain to Childrenâ€™s Hospital St. Hedwig in Regensburg, Germany.](https://doi.org/10.5281/zenodo.7711412)


```{r include=FALSE}
#| label: Loading pediatric appendicitis dataset
pedia_appen_raw <- read_excel("data/app_data.xlsx",
  sheet = 1
)
```



- Taking a look at the dataset

```{r echo=FALSE}
#| label: Sample data view

pedia_appen_raw |>
  drop_na(Sex, US_Performed, Severity, Management, Diagnosis) |>
  select(Sex, US_Performed, Severity, Management, Diagnosis) |>
  glimpse() |>
  sample_n(10) |>
  kable(
    caption = "First Ten Rows of the Pediatric Patients Dataset"
  ) 


missing_values <- sum(is.na(pedia_appen_raw))

cat("Number of missing values is", missing_values)
```



## Some Plots

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Distribution of the Patients's Age by Gender"}
#| label: A histogram for the age


pedia_appen_raw |>
  drop_na(Age, Sex) |>
  ggplot(aes(x = Age)) +
  geom_histogram(color = "white", binwidth = .3) +
  labs(
    x = "Age",
    y = "Count",
    title = "Distribution of the Patients's Age by Gender"
  ) +
  facet_wrap(~Sex) +
  theme_minimal()


pedia_appen_raw |>
  drop_na(Age, Sex) |>
  group_by(Sex) |>
  summarize(
    Mean = round(mean(Age), 2)
  ) |>
  kable(
    caption = "The Mean Age of Patients By Gender"
  ) 
```



 
```{r echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Prevalence of Appendicitis by Gender"}
#| label: A Bar plot for the Diagnosis


pedia_appen_raw |>
  drop_na(Sex, Diagnosis) |>
  ggplot(aes(x = Diagnosis, fill = Sex)) +
  geom_bar(position = "fill") +
  labs(
    x = "Diagnosis",
    y = "Prop",
    title = "prevalence of Appendicitis by Gender",
    fill = "Gender"
  ) +
  theme(
    legend.position = c(0.4, 0.93),
    legend.direction = "horizontal",
    legend.key.size = unit(0.2, "cm"),
    legend.key.height = unit(0.1, "cm"),
    legend.text.align = 0,
    legend.background = element_rect(color = "black", linewidth = 0.2),
    legend.text = element_text(size = 7),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
    plot.caption = element_text(size = 8, hjust = 0)
  ) +
  guides(fill = guide_legend(nrow = 1)) +
  theme_minimal() +
  scale_fill_colorblind()

pedia_appen_raw |>
  drop_na(Sex, Diagnosis) |>
  count(Sex, Diagnosis) |>
  group_by(Sex) |>
  mutate(
    p = round(n / sum(n), 2)
  ) |>
  kable(
    caption = "Prevalence of Appendicitis by Gender"
  )
```



- Figure 2 shows that the prevalence in appendicitis is more males than females, which is consistent with existing findings, but it is not that substantial.

```{r fig.cap="Alvarado Risk Score .vs Appendicitis Diagnosis"}
pedia_appen_raw |>
  drop_na(Alvarado_Score, Diagnosis, Severity) |>
  ggplot(aes(x = Diagnosis, y = Alvarado_Score)) +
  geom_boxplot(aes(fill = Diagnosis)) +
  labs(
    title = "Alvarado Risk Score .vs Appendicitis Diagnosis",
    subtitle = "Diagnosis by Severity",
    x = "Diagnosis",
    y = "Avarado Score"
  ) +
  facet_wrap(~Severity) +
  scale_fill_colorblind() +
  theme_minimal()

pedia_appen_raw |>
  drop_na(Alvarado_Score, Diagnosis) |>
  group_by(Diagnosis) |>
  summarize(
    mean = round(mean(Alvarado_Score), 2),
    median = round(median(Alvarado_Score), 2)
  ) |>
  kable(
    caption = "Alvarado Risk Score .vs Appendicitis Diagnosis"
  )
```

- Alvarado score is a system that have been developed to identify people who are likely to have appendicitis, as a score below 5 suggests against a diagnosis of appendicitis, while a score of 7 or more is predictive of acute appendicitis, but it is performance varies. Here, the severity of the diagnosis was added to see if the score also differed.



```{r fig.cap="Appendix Diameter .vs Appendicitis Type of Management"}
pedia_appen_raw |>
  drop_na(Appendix_Diameter, Management, Severity) |>
  ggplot(aes(y = Management, x = Appendix_Diameter)) +
  geom_boxplot(aes(fill = Management)) +
  labs(
    title = "Appendix Diameter .vs Appendicitis Type of Management",
    subtitle = "Media Diameter",
    x = "Management",
    y = "Appendix Diameter"
  ) +
  facet_wrap(~Severity) +
  scale_fill_colorblind() +
  theme_minimal()

pedia_appen_raw |>
  drop_na(Appendix_Diameter, Diagnosis) |>
  ggplot(aes(x = Appendix_Diameter)) +
  geom_histogram(binwidth = 0.3) +
  labs(
    title = "Distribution of Appendix Diameter By Diagnosis",
    x = "Diagnosis",
    y = "Appendix Diameter"
  ) +
  facet_wrap(~Diagnosis) +
  scale_fill_colorblind() +
  theme_minimal()

pedia_appen_raw |>
  drop_na(Appendix_Diameter, Diagnosis) |>
  group_by(Diagnosis) |>
  summarize(
    mean = round(mean(Appendix_Diameter), 2),
    median = round(median(Appendix_Diameter), 2)
  ) |>
  kable(
    caption = "Mean of Appendix Diameter By Appendicitis Management"
  )
```



- 
```{r}
#| label: Severity of the appendicits


pedia_appen_raw |>
  drop_na(Severity, Appendicular_Abscess) |>
  ggplot(aes(x = Severity, fill = Appendicular_Abscess)) +
  geom_bar(position = "fill") +
  labs(
    x = "Severity",
    y = "Prop",
    title = "Severity of Appendicitis .vs Appendicular Abscess",
    col = "Appendicular Abscess"
  ) +
  theme_minimal() +
  scale_fill_colorblind()
```


- In severe cases, abscess can be seen and Figure 4 shows that the proportion is higher for complicated cases.



## Statistical Analysis

### Hypothesis testing

- Since the use of ultrasound is less expensive and less harmful than CT, it might streamline the diagnostic process.

Here we will test if the addition of **appendiceal diameter** will have a discernible difference on the outcome of the diagnosis, the method we will use is hypothesis testing with **randomization**, and set the discernability level to be 0.05 (i.e the level of rejection).

The two populations of interest in this study are pediatric patients who do or do not have appendicitis.

*Let* $p$ *= the true mean of appendix diameter in pediatric patients.

- So our hypotheses are

$H_0: p_{Appendicitis} = p_{no~Appendicitis}$

$H_A: p_{Appendicitis} \ne p_{no~Appendicitis}$



```{r echo=FALSE, warning=FALSE}
#| label: hypothsis testing with randomization between diagnosis and appendix diameter

set.seed(202025)

pedia_appen_test2 <- pedia_appen_raw |>
  drop_na(Appendix_Diameter, Diagnosis)

obs_stat2 <- pedia_appen_test2 |>
  specify(Appendix_Diameter ~ Diagnosis) |>
  calculate(stat = "diff in means", order = c("appendicitis", "no appendicitis"))

null_dist2 <- pedia_appen_test2 |>
  specify(Appendix_Diameter ~ Diagnosis) |>
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "permute") |>
  calculate(stat = "diff in means", order = c("appendicitis", "no appendicitis"))

pvale2 <- null_dist2 |>
  get_p_value(obs_stat = obs_stat2, direction = "two sided")
```


- With a p-value of `r round(pvale2, 2)`, which is smaller than the discernability level of 0.05, we reject the null hypothesis. The data provide convincing evidence that there is a difference between the mean appendix diameters of pediatric patients with and without appendicitis.



```{r echo=FALSE}
#| label: confidence interval

set.seed(202025)

boot_dist2 <- pedia_appen_test2 |>
  specify(response = Appendix_Diameter, explanatory = Diagnosis) |>
  generate(reps = 1000, type = "bootstrap") |>
  calculate(stat = "diff in means", order = c("appendicitis", "no appendicitis"))

ci2 <- boot_dist2 |>
  get_ci()

ci2 |>
  kable(
    digits = 2,
    caption = "95% confidence interval for the difference in mean between \n patients diagnosed with appendicitis or no appendicitis."
  )
```



- *We are 95% confident that the mean diameter of the appendix in pediatric patients with "appendicitis" is 3.37 to 3.94 greater than in pediatric patients without appendicitis..*




### Modeling

- To help the health workers make more informed decisions (i.e, Accurately diagnosing the appendicitis) we would use machine learning.

We will build Supervised explainable models like logistic regression then test and validate the model.


For evaluation metrics will use **Cross Validation** as way to build the model then we would use **ROC** to check the models precision and accuracy.



- Model 1: Logistic Regression with Alvarado Score
```{r include=FALSE}
#| label: preparing the dataset for the model
set.seed(22025)

pedia_appen1 <- pedia_appen_raw |>
  drop_na(
    Diagnosis, Alvarado_Score, US_Performed,
    Appendix_Diameter, Weight, BMI
  ) |>
  mutate(
    Diagnosis = factor(
      Diagnosis
    ),
    US_Performed = factor(
      US_Performed
    ),
    BMI = as.numeric(BMI)
  )

unique(pedia_appen1$US_Performed)

pedia_appen_split <- initial_split(pedia_appen1, prop = 0.8)

pedia_appen_training <- training(pedia_appen_split)
pedia_appen_test <- testing(pedia_appen_split)
```

```{r include=FALSE}
#| label: training the model0
set.seed(22025)

pedia_appen_model <- logistic_reg() |>
  fit(Diagnosis ~ Alvarado_Score, data = pedia_appen_training)

tidy(pedia_appen_model)
```

```{r echo=FALSE}
#| label: Testing pediatric appendicitis model0

pedia_appen_aug <- augment(pedia_appen_model,
  new_data = pedia_appen_test
)

pedia_appen_aug |>
  sample_n(10) |>
  clean_names() |>
  select(pred_class, pred_appendicitis, pred_no_appendicitis, alvarado_score) |>
  kable(
    digits = 2,
    caption = "A Model to Diagnose Appendicitis With Avarado Score"
  )
```



```{r echo=FALSE}
#| label: model evaluation0

pedia_appen_aug |>
  count(.pred_class, Diagnosis) |>
  arrange(Diagnosis) |>
  group_by(Diagnosis) |>
  mutate(
    p = round(n / sum(n), 2),
    decision = case_when(
      .pred_class == "appendicitis" & Diagnosis == "appendicitis" ~ "True positive",
      .pred_class == "appendicitis" & Diagnosis == "no appendicitis" ~ "False positive",
      .pred_class == "no appendicitis" & Diagnosis == "appendicitis" ~ "False negative",
      .pred_class == "no appendicitis" & Diagnosis == "no appendicitis" ~ "True negative"
    )
  ) |>
  kable(
    caption = "Precision and Accuracy of Model to Diagnose Appendicitis With Avarado Score"
  )
```

```{r echo=FALSE}
#| label: model eval0

pedia_appen_roc <-
  roc_curve(
    pedia_appen_aug,
    truth = Diagnosis,
    .pred_appendicitis
  )

ggplot(pedia_appen_roc, aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(
    title = "Precision and Accuracy of Model to Diagnose Appendicitis With Avarado Score",
    subtitle = "Rate of TP and TN"
  )
```




- Model 2: Multivariate Logistic Regression
```{r include=FALSE}
#| label: training the model
set.seed(22025)
pedia_appen_model1 <- logistic_reg() |>
  fit(Diagnosis ~ Alvarado_Score +
    Appendix_Diameter + Weight + BMI, data = pedia_appen_training)


tidy(pedia_appen_model1)
```

```{r echo=FALSE}
#| label: Testing pediatric appendicitis model

pedia_appen_aug1 <- augment(pedia_appen_model1,
  new_data = pedia_appen_test
)

pedia_appen_aug1 |>
  sample_n(10) |>
  clean_names() |>
  select(pred_class, pred_appendicitis, pred_no_appendicitis) |>
  kable(
    digits = 2,
    caption = "A Model to Diagnose Appendicitis With Avarado Score, Appendix_Diameter, Weight and BMI"
  )
```

```{r echo=FALSE}
#| label: model evaluation

pedia_appen_aug1 |>
  count(.pred_class, Diagnosis) |>
  arrange(Diagnosis) |>
  group_by(Diagnosis) |>
  mutate(
    p = round(n / sum(n), 2),
    decision = case_when(
      .pred_class == "appendicitis" & Diagnosis == "appendicitis" ~ "True positive",
      .pred_class == "appendicitis" & Diagnosis == "no appendicitis" ~ "False positive",
      .pred_class == "no appendicitis" & Diagnosis == "appendicitis" ~ "False negative",
      .pred_class == "no appendicitis" & Diagnosis == "no appendicitis" ~ "True negative"
    )
  ) |>
  kable(
    caption = "Precision and Accuracy of Model to Diagnose Appendicitis With Avarado Score, Appendix_Diameter, Weight and BMI"
  )
```

```{r echo=FALSE}
#| label: model eval

pedia_appen_roc1 <-
  roc_curve(
    pedia_appen_aug1,
    truth = Diagnosis,
    .pred_appendicitis
  )

ggplot(pedia_appen_roc1, aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(
    title = "Precision and Accuracy of Model to Diagnose Appendicitis With Avarado Score",
    subtitle = "Rate of TP and TN"
  )
```



```{r echo=FALSE, fig.cap="Comparing the Accuracy and Precision of both Models"}
#| label: Comparing models

pedia_appen_roc <- pedia_appen_roc |>
  mutate(model = "Model1")

pedia_appen_roc1 <- pedia_appen_roc1 |>
  mutate(model = "Model2")
bind_rows(
  pedia_appen_roc,
  pedia_appen_roc1
) |>
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal()
```


- The second model that used **Ultra-sonography** results showed lower **False positives and negatives**. per table above.


### Diagnosing

Predicting a case with these values Alvarado Score = 6, Appendix Diameter = 10, Weight = 30, BMI = 20, using out second model we built.

```{r echo=FALSE}
#| label: Predicting appendicitis outcome

case <- tibble(Alvarado_Score = 6, Appendix_Diameter = 10, Weight = 30, BMI = 20)


predict(pedia_appen_model1, case, type="prob") |>
  kable(
    caption = "Prediction Probabilities for a Sample Case"
  )
```




# Limitations

While our analysis provides valuable insights, there are limitations that we should note:

1- Generalizability: The data set is derived from a single hospital cohort, which may limit the generalizability of our findings.

2- Model assumptions: Logistic regression assumes linearity between predictors and log odds of the outcome, which we did not test.

3- Potential biases: We do not address the possibility of selection bias and measurement error in clinical assessments.

4- Data quality: Missing values and potential data entry errors could affect model performance.
Future work could explore validation of the results with external datasets.



# Conclusion

This analysis demonstrates an approach to diagnosing pediatric appendicitis by combining statistical analysis with machine learning. Our findings explore the potential of combining clinical scores with imaging results to improve diagnostic accuracy. We welcome any constructive feedback and collaboration to further refine this analysis. **If you have any suggestions or would like to collaborate, please contact us via the discussion forum or email**.

